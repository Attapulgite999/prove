base_model: Qwen/Qwen2.5-7B-Instruct
model_type: AutoModelForCausalLM
tokenizer_type: AutoTokenizer
trust_remote_code: true

# LoRA configuration
adapter: lora
lora_r: 8
lora_alpha: 16
lora_dropout: 0.05
lora_target_linear: true

# Training settings
load_in_8bit: true
gradient_checkpointing: true
bf16: true
fp16: false

# Dataset configuration
datasets:
  - path: ./data/medalpaca.json
    type: alpaca
    ds_type: json

# Training parameters
sequence_len: 256
micro_batch_size: 1
gradient_accumulation_steps: 32
num_epochs: 3
learning_rate: 5.0e-5
lr_scheduler: cosine
warmup_steps: 100
optimizer: adamw_bnb_8bit

# Output and logging
output_dir: ./outputs/medical_qwen_axolotl
logging_steps: 10
save_steps: 50
save_total_limit: 2
report_to: []

# Evaluation (optional)
eval_steps: 50
evaluation_strategy: steps

# Data processing
preprocessing_num_workers: 2
overwrite_cache: true

# Model specific
chat_template: qwen2_5