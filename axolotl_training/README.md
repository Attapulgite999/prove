# ğŸš€ Fine-tuning Qwen2.5 con Axolotl

Questo progetto implementa un approccio piÃ¹ stabile per il fine-tuning del modello Qwen2.5-7B-Instruct utilizzando Axolotl invece di LLaMA Factory.

## ğŸ“‹ Prerequisiti

- Google Colab con GPU (raccomandato T4 o superiore)
- Google Drive per salvare i risultati
- Connessione internet stabile

## ğŸ“ Struttura del Progetto

```
axolotl_training/
â”œâ”€â”€ config/
â”‚   â””â”€â”€ qwen_axolotl.yaml    # Configurazione Axolotl
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ medalpaca.json       # Dataset di training
â”‚   â””â”€â”€ dataset_info.json    # Metadati del dataset
â”œâ”€â”€ colab_axolotl.ipynb      # Notebook Colab principale
â””â”€â”€ README.md               # Questa guida
```

## ğŸš€ Come Utilizzare

### 1. Apri il Notebook su Colab

1. Vai su [Google Colab](https://colab.research.google.com/)
2. Carica il file `colab_axolotl.ipynb`
3. Abilita la GPU: Runtime â†’ Change runtime type â†’ T4 GPU

### 2. Esegui le Celle in Ordine

1. **Cella 0**: Verifica ambiente (GPU, RAM, CPU)
2. **Cella 1**: Setup ambiente e clonazione repository
3. **Cella 2**: Installazione Axolotl e dipendenze
4. **Cella 3**: Avvio del training
5. **Cella 4**: Conversione in GGUF (dopo il training)
6. **Cella 5**: Download del modello finale

### 3. Monitora il Training

Il training mostrerÃ :
- Progress bar con loss e step
- Utilizzo GPU e memoria
- Tempi stimati

## âš™ï¸ Configurazione

Il file `config/qwen_axolotl.yaml` contiene tutti i parametri di training:

- **Modello**: Qwen/Qwen2.5-7B-Instruct
- **Tecnica**: LoRA (r=8, alpha=16)
- **Dataset**: 500 esempi medici (medalpaca)
- **Epoche**: 3
- **Batch size**: 1 (con gradient accumulation 32)
- **Sequence length**: 256 token
- **Learning rate**: 5e-5

## ğŸ“Š Risultati Attesi

- **Durata**: ~2-4 ore su T4 GPU
- **Utilizzo memoria**: ~12-14GB
- **Output**: Modello LoRA + GGUF per LM Studio

## ğŸ”§ Troubleshooting

### Errore di Memoria GPU
- Riduci `micro_batch_size` a 1
- Aumenta `gradient_accumulation_steps`

### Errore di Timeout Colab
- Il notebook include un keep-alive thread
- Monitora la sessione regolarmente

### Problemi con il Dataset
- Verifica che i file `medalpaca.json` esistano
- Controlla il formato JSON (instruction, input, output)

## ğŸ“ˆ Ottimizzazioni Future

Una volta che il training base funziona, puoi aggiungere:
- **Unsloth**: Per accelerare il training
- **Flash Attention**: Per prestazioni migliori
- **Dataset piÃ¹ grande**: Per risultati migliori

## ğŸ¤ Supporto

Se riscontri problemi:
1. Controlla i log di errore
2. Verifica la configurazione YAML
3. Assicurati che la GPU sia abilitata

Buon fine-tuning! ğŸ‰